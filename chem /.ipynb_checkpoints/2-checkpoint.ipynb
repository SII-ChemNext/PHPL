{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "\n",
    "from loader import MoleculeDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from model import GNN_graphpred, GNN_Bayes\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "from splitters import scaffold_split\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "import pandas \n",
    "import math \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from model import MLPregression \n",
    "from sklearn.manifold import TSNE \n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "K_1 = 16846\n",
    "K_2 = 17.71 # 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#分开的\n",
    "def r2_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_true: torch.Tensor, shape (N,)\n",
    "    y_pred: torch.Tensor, shape (N,)\n",
    "    \"\"\"\n",
    "    y_true_mean = torch.mean(y_true)\n",
    "    ss_tot = torch.sum((y_true - y_true_mean) ** 2)\n",
    "    ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2.item()\n",
    "\n",
    "def eval(args, model, dataset, device, loader): # para_contrastive_loss,\n",
    "    model.eval()\n",
    "\n",
    "    y_scores = []\n",
    "    y_scores_cl = []\n",
    "    y_scores_vdss = []\n",
    "    y_scores_t1_2 = []\n",
    "    \n",
    "    y_cl = []\n",
    "    y_vdss = []\n",
    "    y_t1_2 = []\n",
    "    y_auc = []\n",
    "\n",
    "    for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_log = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "            # pred_log, std= model(batch.x, batch.edge_index, batch.edge_attr, batch.batch,1, False)\n",
    "            # cl_pred_log = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch, 0, True)\n",
    "            # vdss_pred_log = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch, 1, True)\n",
    "            # t1_2_pred_log = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch, 2, True)\n",
    "            # pred_log = torch.log(pred)\n",
    "            \n",
    "            cl_y = batch.cl_y.view(batch.cl_y.size(0), 1)\n",
    "            vdss_y = batch.vdss_y.view(batch.vdss_y.size(0), 1)\n",
    "            t1_2_y = batch.t1_2_y.view(batch.t1_2_y.size(0), 1)\n",
    "\n",
    "            y_scores.append(pred_log)\n",
    "            # y_scores_cl.append(cl_pred_log)\n",
    "            # y_scores_vdss.append(vdss_pred_log)\n",
    "            # y_scores_t1_2.append(t1_2_pred_log)\n",
    "            \n",
    "            y_cl.append(cl_y)\n",
    "            y_vdss.append(vdss_y)\n",
    "            y_t1_2.append(t1_2_y)\n",
    "            \n",
    "    y_scores = torch.cat(y_scores, dim=0)\n",
    "    # y_scores_cl = torch.cat(y_scores_cl, dim=0)\n",
    "    # y_scores_vdss = torch.cat(y_scores_vdss, dim=0)\n",
    "    # y_scores_t1_2 = torch.cat(y_scores_t1_2, dim=0)\n",
    "    \n",
    "    y_cl = torch.cat(y_cl, dim=0)\n",
    "    y_vdss = torch.cat(y_vdss, dim=0)\n",
    "    y_t1_2 = torch.cat(y_t1_2, dim=0)\n",
    "    \n",
    "    if dataset == 'cl':\n",
    "        loss = criterion(y_scores, y_cl).cpu().detach().item()\n",
    "        r2 = r2_score(y_cl, y_scores)\n",
    "    elif dataset == 'vdss':\n",
    "        loss = criterion(y_scores, y_vdss).cpu().detach().item()\n",
    "        r2 = r2_score(y_vdss, y_scores)\n",
    "    elif dataset == 't1_2':\n",
    "        loss = criterion(y_scores, y_t1_2).cpu().detach().item()\n",
    "        r2 = r2_score(y_t1_2, y_scores)\n",
    "    elif dataset == '4':\n",
    "        loss_cl = criterion(y_scores_cl, y_cl).cpu().detach().item()\n",
    "        r2_cl = r2_score(y_cl, y_scores_cl)\n",
    "        loss_vdss = criterion(y_scores_vdss, y_vdss).cpu().detach().item()\n",
    "        r2_vdss =r2_score(y_vdss, y_scores_vdss)\n",
    "        loss_t1_2 = criterion(y_scores_t1_2, y_t1_2).cpu().detach().item()\n",
    "        r2_t1_2 =r2_score(y_t1_2, y_scores_t1_2)\n",
    "    \n",
    "    # return loss_cl,loss_vdss,loss_t1_2\n",
    "    return loss,r2\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch implementation of pre-training of graph neural networks')\n",
    "    parser.add_argument('--device', type=int, default=0,\n",
    "                        help='which gpu to use if any (default: 0)')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, ## 32: 1.009 # 64: \n",
    "                        help='input batch size for training (default: 32)')\n",
    "    parser.add_argument('--epochs', type=int, default=100, # 60 # 1500\n",
    "                        help='number of epochs to train (default: 100)')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, # 0.0001\n",
    "                        help='learning rate (default: 0.001)')\n",
    "    parser.add_argument('--lr_scale', type=float, default=1,\n",
    "                        help='relative learning rate for the feature extraction layer (default: 1)')\n",
    "    parser.add_argument('--decay', type=float, default=1e-9,\n",
    "                        help='weight decay (default: 0)')\n",
    "    parser.add_argument('--num_layer', type=int, default=5,\n",
    "                        help='number of GNN message passing layers (default: 5).')\n",
    "    parser.add_argument('--emb_dim', type=int, default=300,\n",
    "                        help='embedding dimensions (default: 300)')\n",
    "    parser.add_argument('--dropout_ratio', type=float, default=0.0,\n",
    "                        help='dropout ratio (default: 0.5)')\n",
    "    parser.add_argument('--graph_pooling', type=str, default=\"mean\",\n",
    "                        help='graph level pooling (sum, mean, max, set2set, attention)')\n",
    "    parser.add_argument('--JK', type=str, default=\"last\",\n",
    "                        help='how the node features across layers are combined. last, sum, max or concat')\n",
    "    parser.add_argument('--gnn_type', type=str, default=\"gin\")\n",
    "    parser.add_argument('--dataset', type=str, default = '4', help='root directory of dataset. For now, only classification.')\n",
    "    \n",
    "    parser.add_argument('--split', type = str, default=\"scaffold\", help = \"random or scaffold or random_scaffold\")\n",
    "    parser.add_argument('--eval_train', type=int, default=0, help='evaluating training or not')\n",
    "    parser.add_argument('--num_workers', type=int, default=4, help='number of workers for dataset loading')\n",
    "    parser.add_argument('--scheduler', action=\"store_true\", default=False)\n",
    "    parser.add_argument('--experiment_name', type=str, default=\"graphmae\")\n",
    "    parser.add_argument('--seed', type=int, default=42, help = \"Seed for splitting the dataset.\")\n",
    "    parser.add_argument('--runseed', type=int, default=42, help = \"Seed for minibatch selection, random initialization.\")\n",
    "    \n",
    "    parser.add_argument('--input_vdss_model_file', type=str, default = \n",
    "                        '../results/20250324/finetune_mask/vdss/lr_0.001_decay_1e-05_bz_64_seed_42_dropout_0.0/best_model.pth', help='filename to read the model (if there is any)')\n",
    "    parser.add_argument('--input_t1_2_model_file', type=str, default = \n",
    "                        '../results/20250312/finetune_l2/t1_2/lr_0.001_decay_1e-05_bz_128_seed_42_dropout_0.0/best_model.pth', help='filename to read the model (if there is any)')\n",
    "    parser.add_argument('--input_cl_model_file', type=str, default = \n",
    "                        '../results/20250312/finetune_l2/cl/lr_0.001_decay_1e-05_bz_128_seed_42_dropout_0.0/best_model.pth', help='filename to read the model (if there is any)')\n",
    "    parser.add_argument('--input_model_file', type=str, default = \n",
    "                        '../checkpoint/20250309/lr_0.001_decay_0_bz_256_dropout_0.0/best_model.pth', help='filename to read the model (if there is any)')\n",
    "    \n",
    "    ## add some argument \n",
    "    parser.add_argument('--dataset_type', type=int, default=1)\n",
    "    parser.add_argument('--save', type=str, default='../results/20250407/finetune_mask/')\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    torch.manual_seed(args.runseed)\n",
    "    np.random.seed(args.runseed)\n",
    "    # args.seed = args.runseed \n",
    "    args.experiment_name = 'lr'+'_'+str(args.lr)+'_'+'decay'+'_'+str(args.decay)+'_'+'bs'+'_'+str(args.batch_size)+'_'+'drop'+'_'+str(args.dropout_ratio)\n",
    "    # os.makedirs(args.save+args.experiment_name, exist_ok= True)\n",
    "    \n",
    "    motif_list_path = '../dataset_reg/motif_list.pkl'\n",
    "    print(f\"文件 {motif_list_path} 存在，从文件中加载 motif_list...\")\n",
    "    with open(motif_list_path, 'rb') as f:\n",
    "        motif_list = pickle.load(f)\n",
    "\n",
    "    device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.runseed)\n",
    "\n",
    "    if args.dataset == \"4\":\n",
    "        num_tasks = 1\n",
    "        valid_dataset_name = \"4_valid\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset name.\")\n",
    "\n",
    "    ## set up pk dataset \n",
    "    valid_dataset = MoleculeDataset(\"../dataset_reg/\"+valid_dataset_name, dataset=valid_dataset_name, motif_list=motif_list)\n",
    "     \n",
    "    val_loader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False, num_workers = args.num_workers)\n",
    "    \n",
    "    model = GNN_graphpred(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type, classnum=args.dataset_type)\n",
    "    # t1_2_model = GNN_graphpred(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type, classnum=args.dataset_type)\n",
    "    # cl_model = GNN_graphpred(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type, classnum=args.dataset_type)\n",
    "    \n",
    "    root_folder = args.save\n",
    "    sub_folders = ['cl', 'vdss', 't1_2']\n",
    "\n",
    "    for sub in sub_folders:\n",
    "        sub_path = os.path.join(root_folder, sub)\n",
    "        # 遍历 sub_path 内的每个超参数模型文件夹\n",
    "        for model_folder in os.listdir(sub_path):\n",
    "            model_path = os.path.join(sub_path, model_folder)\n",
    "            if os.path.isdir(model_path):\n",
    "                best_model_file = os.path.join(model_path, 'best_model.pth')\n",
    "                if os.path.exists(best_model_file):\n",
    "                    print(\"load pretrained model from:\", best_model_file)\n",
    "                    model.load_state_dict(torch.load(best_model_file, map_location=device)[f'model_{sub}'])\n",
    "                    model.to(device)\n",
    "                    loss = eval(args, model, sub, device, val_loader)\n",
    "                    new_data = pd.DataFrame({f'{sub}_loss':[loss], \n",
    "                             }, index=[f'{sub}+{model_folder}'])\n",
    "    \n",
    "                    save_path = args.save+\"result.csv\"\n",
    "\n",
    "                    # 检查文件是否存在，存在则读取并追加新数据，否则新建\n",
    "                    if os.path.exists(save_path):\n",
    "                        existing_data = pd.read_csv(save_path, index_col='experiment_name')\n",
    "                        # 使用 pd.concat 合并数据\n",
    "                        updated_data = pd.concat([existing_data, new_data])\n",
    "                    else:\n",
    "                        updated_data = new_data\n",
    "\n",
    "                    # 保存到CSV，保留索引（experiment_name作为行标签）\n",
    "                    updated_data.to_csv(save_path, index_label='experiment_name')\n",
    "                else:\n",
    "                    print(\"没有找到 best_model.pth:\", model_path)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 ../dataset_reg/motif_list.pkl 存在，从文件中加载 motif_list...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/inspire/hdd/ws-f4d69b29-e0a5-44e6-bd92-acf4de9990f0/public-project/wenyihao-240208090182/PEMAL-master1/chem/loader.py:422: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_data = torch.load(self.processed_paths[0])\n",
      "/opt/conda/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/tmp/ipykernel_528/1158636095.py:155: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_file, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pretrained model from: ../results/20250703/finetune_1+2_mask0.5_together/lr_0.001_decay_1e-05_bs_64_dropout_0.0_beta_1.0/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 2/2 [00:00<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pretrained model from: ../results/20250703/finetune_1+2_mask0.5_together/lr_0.001_decay_1e-05_bs_32_dropout_0.0_beta_0.0/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 16.21it/s]\n",
      "/tmp/ipykernel_528/1158636095.py:155: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_file, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pretrained model from: ../results/20250703/finetune_1+2_mask0.5_together/lr_0.001_decay_1e-05_bs_64_dropout_0.0_beta_0.0/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 13.80it/s]\n",
      "/tmp/ipykernel_528/1158636095.py:155: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_file, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pretrained model from: ../results/20250703/finetune_1+2_mask0.5_together/lr_0.001_decay_1e-05_bs_32_dropout_0.0_beta_1.0/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 15.23it/s]\n"
     ]
    }
   ],
   "source": [
    "#合并的\n",
    "def r2_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_true: torch.Tensor, shape (N,)\n",
    "    y_pred: torch.Tensor, shape (N,)\n",
    "    \"\"\"\n",
    "    y_true_mean = torch.mean(y_true)\n",
    "    ss_tot = torch.sum((y_true - y_true_mean) ** 2)\n",
    "    ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2.item()\n",
    "\n",
    "def eval(args, model, device, loader): # para_contrastive_loss,\n",
    "    model.eval()\n",
    "\n",
    "    y_scores = []\n",
    "    y_scores_cl = []\n",
    "    y_scores_vdss = []\n",
    "    y_scores_t1_2 = []\n",
    "    \n",
    "    y_cl = []\n",
    "    y_vdss = []\n",
    "    y_t1_2 = []\n",
    "    y_auc = []\n",
    "\n",
    "    for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_log= model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "            # pred_log, std= model(batch.x, batch.edge_index, batch.edge_attr, batch.batch,1, False)\n",
    "            cl_pred_log = pred_log[:,[0]]\n",
    "            vdss_pred_log = pred_log[:,[1]]\n",
    "            t1_2_pred_log = pred_log[:,[2]]\n",
    "            # pred_log = torch.log(pred)\n",
    "            \n",
    "            cl_y = batch.cl_y.view(batch.cl_y.size(0), 1)\n",
    "            vdss_y = batch.vdss_y.view(batch.vdss_y.size(0), 1)\n",
    "            t1_2_y = batch.t1_2_y.view(batch.t1_2_y.size(0), 1)\n",
    "\n",
    "            # y_scores.append(pred_log)\n",
    "            y_scores_cl.append(cl_pred_log)\n",
    "            y_scores_vdss.append(vdss_pred_log)\n",
    "            y_scores_t1_2.append(t1_2_pred_log)\n",
    "            \n",
    "            y_cl.append(cl_y)\n",
    "            y_vdss.append(vdss_y)\n",
    "            y_t1_2.append(t1_2_y)\n",
    "            \n",
    "    # y_scores = torch.cat(y_scores, dim=0)\n",
    "    y_scores_cl = torch.cat(y_scores_cl, dim=0)\n",
    "    y_scores_vdss = torch.cat(y_scores_vdss, dim=0)\n",
    "    y_scores_t1_2 = torch.cat(y_scores_t1_2, dim=0)\n",
    "    \n",
    "    y_cl = torch.cat(y_cl, dim=0)\n",
    "    y_vdss = torch.cat(y_vdss, dim=0)\n",
    "    y_t1_2 = torch.cat(y_t1_2, dim=0)\n",
    "    \n",
    "    loss_cl = criterion(y_scores_cl, y_cl).cpu().detach().item()\n",
    "    r2_cl = r2_score(y_cl, y_scores_cl)\n",
    "    loss_vdss = criterion(y_scores_vdss, y_vdss).cpu().detach().item()\n",
    "    r2_vdss =r2_score(y_vdss, y_scores_vdss)\n",
    "    loss_t1_2 = criterion(y_scores_t1_2, y_t1_2).cpu().detach().item()\n",
    "    r2_t1_2 =r2_score(y_t1_2, y_scores_t1_2)\n",
    "    \n",
    "    return loss_cl,loss_vdss,loss_t1_2\n",
    "    # return loss,r2\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch implementation of pre-training of graph neural networks')\n",
    "    parser.add_argument('--device', type=int, default=0,\n",
    "                        help='which gpu to use if any (default: 0)')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, ## 32: 1.009 # 64: \n",
    "                        help='input batch size for training (default: 32)')\n",
    "    parser.add_argument('--epochs', type=int, default=100, # 60 # 1500\n",
    "                        help='number of epochs to train (default: 100)')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, # 0.0001\n",
    "                        help='learning rate (default: 0.001)')\n",
    "    parser.add_argument('--lr_scale', type=float, default=1,\n",
    "                        help='relative learning rate for the feature extraction layer (default: 1)')\n",
    "    parser.add_argument('--decay', type=float, default=1e-9,\n",
    "                        help='weight decay (default: 0)')\n",
    "    parser.add_argument('--num_layer', type=int, default=5,\n",
    "                        help='number of GNN message passing layers (default: 5).')\n",
    "    parser.add_argument('--emb_dim', type=int, default=300,\n",
    "                        help='embedding dimensions (default: 300)')\n",
    "    parser.add_argument('--dropout_ratio', type=float, default=0.0,\n",
    "                        help='dropout ratio (default: 0.5)')\n",
    "    parser.add_argument('--graph_pooling', type=str, default=\"mean\",\n",
    "                        help='graph level pooling (sum, mean, max, set2set, attention)')\n",
    "    parser.add_argument('--JK', type=str, default=\"last\",\n",
    "                        help='how the node features across layers are combined. last, sum, max or concat')\n",
    "    parser.add_argument('--gnn_type', type=str, default=\"gin\")\n",
    "    parser.add_argument('--dataset', type=str, default = '4', help='root directory of dataset. For now, only classification.')\n",
    "    \n",
    "    parser.add_argument('--split', type = str, default=\"scaffold\", help = \"random or scaffold or random_scaffold\")\n",
    "    parser.add_argument('--eval_train', type=int, default=0, help='evaluating training or not')\n",
    "    parser.add_argument('--num_workers', type=int, default=4, help='number of workers for dataset loading')\n",
    "    parser.add_argument('--scheduler', action=\"store_true\", default=False)\n",
    "    parser.add_argument('--experiment_name', type=str, default=\"graphmae\")\n",
    "    parser.add_argument('--seed', type=int, default=42, help = \"Seed for splitting the dataset.\")\n",
    "    parser.add_argument('--runseed', type=int, default=42, help = \"Seed for minibatch selection, random initialization.\")\n",
    "    \n",
    "    parser.add_argument('--input_vdss_model_file', type=str, default = \n",
    "                        '../results/20250324/finetune_mask/vdss/lr_0.001_decay_1e-05_bz_64_seed_42_dropout_0.0/best_model.pth', help='filename to read the model (if there is any)')\n",
    "    parser.add_argument('--input_t1_2_model_file', type=str, default = \n",
    "                        '../results/20250312/finetune_l2/t1_2/lr_0.001_decay_1e-05_bz_128_seed_42_dropout_0.0/best_model.pth', help='filename to read the model (if there is any)')\n",
    "    parser.add_argument('--input_cl_model_file', type=str, default = \n",
    "                        '../results/20250312/finetune_l2/cl/lr_0.001_decay_1e-05_bz_128_seed_42_dropout_0.0/best_model.pth', help='filename to read the model (if there is any)')\n",
    "    parser.add_argument('--input_model_file', type=str, default = \n",
    "                        '../checkpoint/20250309/lr_0.001_decay_0_bz_256_dropout_0.0/best_model.pth', help='filename to read the model (if there is any)')\n",
    "    \n",
    "    ## add some argument \n",
    "    parser.add_argument('--dataset_type', type=int, default=1)\n",
    "    parser.add_argument('--save', type=str, default='../results/20250703/finetune_1+2_mask0.5_together/')\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    torch.manual_seed(args.runseed)\n",
    "    np.random.seed(args.runseed)\n",
    "    # args.seed = args.runseed \n",
    "    args.experiment_name = 'lr'+'_'+str(args.lr)+'_'+'decay'+'_'+str(args.decay)+'_'+'bs'+'_'+str(args.batch_size)+'_'+'drop'+'_'+str(args.dropout_ratio)\n",
    "    # os.makedirs(args.save+args.experiment_name, exist_ok= True)\n",
    "    \n",
    "    motif_list_path = '../dataset_reg/motif_list.pkl'\n",
    "    print(f\"文件 {motif_list_path} 存在，从文件中加载 motif_list...\")\n",
    "    with open(motif_list_path, 'rb') as f:\n",
    "        motif_list = pickle.load(f)\n",
    "\n",
    "    device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.runseed)\n",
    "\n",
    "    if args.dataset == \"4\":\n",
    "        num_tasks = 3\n",
    "        valid_dataset_name = \"4_valid\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset name.\")\n",
    "\n",
    "    ## set up pk dataset \n",
    "    valid_dataset = MoleculeDataset(\"../dataset_new_desc/\"+valid_dataset_name, dataset=valid_dataset_name, motif_list=motif_list)\n",
    "     \n",
    "    val_loader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False, num_workers = args.num_workers)\n",
    "    \n",
    "    model = GNN_graphpred(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type, classnum=args.dataset_type)\n",
    "    \n",
    "    root_folder = args.save\n",
    "\n",
    "\n",
    "    for model_folder in os.listdir(root_folder):\n",
    "        model_path = os.path.join(root_folder,model_folder)\n",
    "        if os.path.isdir(model_path):\n",
    "            best_model_file = os.path.join(model_path, 'best_model.pth')\n",
    "            if os.path.exists(best_model_file):\n",
    "                print(\"load pretrained model from:\", best_model_file)\n",
    "                model.load_state_dict(torch.load(best_model_file, map_location=device))\n",
    "                model.to(device)\n",
    "                loss_cl,loss_vdss,loss_t1_2 = eval(args, model, device, val_loader)\n",
    "                new_data = pd.DataFrame({'loss_cl':loss_cl, \n",
    "                                         'loss_vdss':loss_vdss,\n",
    "                                         'loss_t1_2':loss_t1_2\n",
    "                         }, index=[model_folder])\n",
    "\n",
    "                save_path = args.save+\"result.csv\"\n",
    "\n",
    "                # 检查文件是否存在，存在则读取并追加新数据，否则新建\n",
    "                if os.path.exists(save_path):\n",
    "                    existing_data = pd.read_csv(save_path, index_col='experiment_name')\n",
    "                    # 使用 pd.concat 合并数据\n",
    "                    updated_data = pd.concat([existing_data, new_data])\n",
    "                else:\n",
    "                    updated_data = new_data\n",
    "\n",
    "                # 保存到CSV，保留索引（experiment_name作为行标签）\n",
    "                updated_data.to_csv(save_path, index_label='experiment_name')\n",
    "            else:\n",
    "                print(\"没有找到 best_model.pth:\", model_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳 cl_loss 实验: cl+lr_0.01_decay_1e-06_bz_32_seed_42_dropout_0.0，Loss: 0.6789669990539551\n",
      "最佳 vdss_loss 实验: vdss+lr_0.01_decay_1e-10_bz_32_seed_42_dropout_0.3，Loss: 0.6753852963447571\n",
      "最佳 t1_2_loss 实验: t1_2+lr_0.0001_decay_1e-06_bz_32_seed_42_dropout_0.1，Loss: 0.7618190050125122\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取 CSV 文件\n",
    "df = pd.read_csv('../results/20250327/finetune_0/result.csv')  # 这里替换为你的 CSV 文件路径\n",
    "\n",
    "# 处理 NaN 值，填充成一个很大的数，确保不会影响 min 操作\n",
    "df.fillna(float('inf'), inplace=True)\n",
    "\n",
    "# 找到每种实验的最小 loss 及其 index\n",
    "best_cl = df.loc[df['cl_loss'].idxmin(), ['experiment_name', 'cl_loss']]\n",
    "best_vdss = df.loc[df['vdss_loss'].idxmin(), ['experiment_name', 'vdss_loss']]\n",
    "best_t1_2 = df.loc[df['t1_2_loss'].idxmin(), ['experiment_name', 't1_2_loss']]\n",
    "\n",
    "# 打印最佳实验的 index 和 loss\n",
    "print(f\"最佳 cl_loss 实验: {best_cl['experiment_name']}，Loss: {best_cl['cl_loss']}\")\n",
    "print(f\"最佳 vdss_loss 实验: {best_vdss['experiment_name']}，Loss: {best_vdss['vdss_loss']}\")\n",
    "print(f\"最佳 t1_2_loss 实验: {best_t1_2['experiment_name']}，Loss: {best_t1_2['t1_2_loss']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from loader import MoleculeDataset\n",
    "motif_list=[]\n",
    "cl_train_dataset = MoleculeDataset(\"dataset_reg/cl_train\", dataset=\"cl_train\", motif_list=motif_list)\n",
    "cl_valid_dataset = MoleculeDataset(\"dataset_reg/cl_valid\", dataset=\"cl_valid\", motif_list=motif_list)\n",
    "\n",
    "vdss_train_dataset = MoleculeDataset(\"dataset_reg/vdss_train\", dataset=\"vdss_train\", motif_list=motif_list)\n",
    "vdss_valid_dataset = MoleculeDataset(\"dataset_reg/vdss_valid\", dataset=\"vdss_valid\", motif_list=motif_list)\n",
    "\n",
    "t1_2_train_dataset = MoleculeDataset(\"dataset_reg/t1_2_train\", dataset=\"t1_2_train\", motif_list=motif_list)\n",
    "t1_2_valid_dataset = MoleculeDataset(\"dataset_reg/t1_2_valid\", dataset=\"t1_2_valid\", motif_list=motif_list)\n",
    "\n",
    "print(len(cl_train_dataset))\n",
    "print(len(vdss_train_dataset))\n",
    "print(len(t1_2_train_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
